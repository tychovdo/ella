{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3244c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.gitignore', 'classification_image.py', 'sparse', 'requirements.txt', '.git', 'hpc', 'laplace', '__pycache__', 'data_utils', 'tmp', '.ipynb_checkpoints', 'scripts', 'results', 'asdfghjkl', 'notebooks', 'configs', 'data', 'arg_utils.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not 'notebooks' in os.listdir():\n",
    "    os.chdir('..')\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "576daec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a021b01f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/mnist/mixer1/baseline_approx=kron_fixedaug_E=2_N=50000_S=31_seed=711'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/mnist/mixer1/baseline_approx=kron_fixedaug_E=2_N=50000_S=31_seed=711\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/mnist/mixer1/baseline_approx=kron_fixedaug_E=2_N=50000_S=31_seed=711\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/mnist/mixer1/baseline_approx=kron_fixedaug_E=2_N=50000_S=31_seed=711'"
     ]
    }
   ],
   "source": [
    "print(os.listdir('results/mnist/mixer1/baseline_approx=kron_fixedaug_E=2_N=50000_S=31_seed=711'))\n",
    "path = Path('results/mnist/mixer1/baseline_approx=kron_fixedaug_E=2_N=50000_S=31_seed=711')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8c0d4e48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_weights() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         weights[name] \u001b[38;5;241m=\u001b[39m weight\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights\n\u001b[0;32m---> 12\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_weights() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "def get_weights(path):\n",
    "    \n",
    "    weights = {}\n",
    "    for name in os.listdir(path):\n",
    "        with open(path / name, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        weight = data.detach().cpu().numpy()\n",
    "        \n",
    "        weights[name] = weight\n",
    "    return weights\n",
    "        \n",
    "weights = get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bc558fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv_net.0.linear.lin_V.weight.pkl', 'conv_net.0.linear.lin_V.bias.pkl', 'conv_net.0.gconv.conv.bias.pkl', 'conv_net.0.linear.lin_U.weight.pkl', 'conv_net.0.linear.lin_U.bias.pkl', 'conv_net.0.conv.bias.pkl', 'conv_net.0.conv.weight.pkl', 'conv_net.0.gconv.conv.weight.pkl']\n"
     ]
    }
   ],
   "source": [
    "keys = [n for n in weights.keys()]\n",
    "\n",
    "keys_z = [x for x in keys if 'conv_net.0' in x]\n",
    "print(keys_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3cc46195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function `torch_conv_layer_to_affine` takes a `torch.nn.Conv2d` layer `conv`\n",
    "and produces an equivalent `torch.nn.Linear` layer `fc`.\n",
    "Specifically, this means that the following holds for `x` of a valid shape:\n",
    "    torch.flatten(conv(x)) == fc(torch.flatten(x))\n",
    "Or equivalently:\n",
    "    conv(x) == fc(torch.flatten(x)).reshape(conv(x).shape)\n",
    "allowing of course for some floating-point error.\n",
    "\"\"\"\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def torch_conv_layer_to_affine(\n",
    "    conv: torch.nn.Conv2d, input_size: Tuple[int, int]\n",
    ") -> torch.nn.Linear:\n",
    "    w, h = input_size\n",
    "\n",
    "    # Formula from the Torch docs:\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    output_size = [\n",
    "        (input_size[i] + 2 * conv.padding[i] - conv.kernel_size[i]) // conv.stride[i]\n",
    "        + 1\n",
    "        for i in [0, 1]\n",
    "    ]\n",
    "\n",
    "    in_shape = (conv.in_channels, w, h)\n",
    "    out_shape = (conv.out_channels, output_size[0], output_size[1])\n",
    "\n",
    "    fc = nn.Linear(in_features=np.product(in_shape), out_features=np.product(out_shape))\n",
    "    fc.weight.data.fill_(0.0)\n",
    "\n",
    "    # Output coordinates\n",
    "    for xo, yo in range2d(output_size[0], output_size[1]):\n",
    "        # The upper-left corner of the filter in the input tensor\n",
    "        xi0 = -conv.padding[0] + conv.stride[0] * xo\n",
    "        yi0 = -conv.padding[1] + conv.stride[1] * yo\n",
    "\n",
    "        # Position within the filter\n",
    "        for xd, yd in range2d(conv.kernel_size[0], conv.kernel_size[1]):\n",
    "            # Output channel\n",
    "            for co in range(conv.out_channels):\n",
    "                #fc.bias[enc_tuple((co, xo, yo), out_shape)] = conv.bias[co]\n",
    "                for ci in range(conv.in_channels):\n",
    "                    # Make sure we are within the input image (and not in the padding)\n",
    "                    if 0 <= xi0 + xd < w and 0 <= yi0 + yd < h:\n",
    "                        cw = conv.weight[co, ci, xd, yd]\n",
    "                        # Flatten the weight position to 1d in \"canonical ordering\",\n",
    "                        # i.e. guaranteeing that:\n",
    "                        # FC(img.reshape(-1)) == Conv(img).reshape(-1)\n",
    "                        fc.weight[\n",
    "                            enc_tuple((co, xo, yo), out_shape),\n",
    "                            enc_tuple((ci, xi0 + xd, yi0 + yd), in_shape),\n",
    "                        ] = cw\n",
    "\n",
    "    return fc\n",
    "\n",
    "\n",
    "def range2d(to_a, to_b):\n",
    "    for a in range(to_a):\n",
    "        for b in range(to_b):\n",
    "            yield a, b\n",
    "\n",
    "\n",
    "def enc_tuple(tup: Tuple, shape: Tuple) -> int:\n",
    "    res = 0\n",
    "    coef = 1\n",
    "    for i in reversed(range(len(shape))):\n",
    "        assert tup[i] < shape[i]\n",
    "        res += coef * tup[i]\n",
    "        coef *= shape[i]\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def dec_tuple(x: int, shape: Tuple) -> Tuple:\n",
    "    res = []\n",
    "    for i in reversed(range(len(shape))):\n",
    "        res.append(x % shape[i])\n",
    "        x //= shape[i]\n",
    "\n",
    "    return tuple(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8a1dd34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 32)\n",
      "(8, 4, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "linear = weights['conv_net.0.linear.lin_V.weight.pkl']\n",
    "print(linear.shape)\n",
    "conv = weights['conv_net.0.conv.weight.pkl']\n",
    "print(conv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "40ddbc22",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m linear_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(linear\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], linear\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlinear_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(linear)\n\u001b[1;32m      4\u001b[0m conv_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(conv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], conv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], conv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m conv_layer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(conv)\n",
      "File \u001b[0;32m/vol/bitbucket/tv21/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1635\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1636\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1637\u001b[0m                         \u001b[38;5;241m.\u001b[39mformat(torch\u001b[38;5;241m.\u001b[39mtypename(value), name))\n\u001b[1;32m   1638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "linear_layer = nn.Linear(linear.shape[1], linear.shape[0], bias=False)\n",
    "linear_layer.weight = torch.tensor(linear)\n",
    "\n",
    "conv_layer = nn.Conv2d(conv.shape[1], conv.shape[0], conv.shape[-2:], bias=False)\n",
    "conv_layer.weight = torch.tensor(conv)\n",
    "\n",
    "linconv_layer = torch_conv_layer_to_affine(conv_layer, (32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde629b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61debd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fcde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
