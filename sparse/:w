import torch
from torch import nn
from math import sqrt
import numpy as np 

from .operation import Operation


# NOTE: set here
CASE = 'expand'  # or as previosuly 'reduce'

class SLinearUExt(Operation):
    @staticmethod
    def batch_grads_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        return batch_grads.sum(tuple(range(1, in_data.ndim-1)))

    @staticmethod
    def batch_grads_bias(module, out_grads):
        return out_grads.sum(tuple(range(1, out_grads.ndim-1)))

    @staticmethod
    def batch_grads_kron_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        in_data = in_data.sum(tuple(range(1, in_data.ndim-1)))
        out_grads = out_grads.mean(tuple(range(1, out_grads.ndim-1)))
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        return batch_grads

    @staticmethod
    def batch_grads_kron_bias(module, out_grads):
        return out_grads.sum(tuple(range(1, out_grads.ndim-1)))

    @staticmethod
    def cov_diag_weight(module, in_data, out_grads):
        # efficient reduction for augmentation and weight-sharing
        in_data = in_data.mean(tuple(range(1, in_data.ndim-1, 1)))
        out_grads = out_grads.sum(tuple(range(1, out_grads.ndim-1, 1)))
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        return batch_grads.square().sum(dim=0)

    @staticmethod
    def cov_diag_bias(module, out_grads):
        return out_grads.sum(tuple(range(1, out_grads.ndim-1)))

    @staticmethod
    def cov_kron_A(module, in_data):
        dim = np.prod(in_data.shape[1:-1])

        deriv = module.regression._deriv

        print(1, in_data.shape)
        in_data = in_data.view(*in_data.shape[:2], module.in_channels, -1) @ deriv.T
        print(2, in_data.shape)
        in_data = in_data.view(*in_data.shape[:2], -1)
        print(3, in_data.shape)

        if CASE == 'reduce':
            in_data = in_data.mean(tuple(range(1, in_data.ndim-1, 1))) * sqrt(dim)
        elif CASE == 'expand':
            in_data = in_data.flatten(end_dim=-2) / sqrt(dim)

        m = torch.matmul(in_data.T, in_data)

        return m

    @staticmethod
    def cov_kron_B(module, out_grads):
        deriv = module.regression._deriv

        if CASE == 'reduce':
            out_grads = out_grads.sum(tuple(range(1, out_grads.ndim-1, 1)))
        elif CASE == 'expand':
            out_grads = out_grads.flatten(end_dim=-2)

        m = torch.matmul(out_grads.T, out_grads)

        return m

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        dim = np.prod(in_data1.shape[1:-1])
        assert dim == np.prod(in_data2.shape[1:-1])
        if in_data1.ndim > 2:
            in_data1 = in_data1.mean(tuple(range(1, in_data1.ndim-1))) * sqrt(dim)
            in_data2 = in_data2.mean(tuple(range(1, in_data2.ndim-1))) * sqrt(dim)
        return torch.matmul(in_data1, in_data2.T)  # n x n

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        if out_grads1.ndim > 2:
            out_grads1 = out_grads1.sum(tuple(range(1, out_grads1.ndim-1)))
            out_grads2 = out_grads2.sum(tuple(range(1, out_grads2.ndim-1)))
        return torch.matmul(out_grads1, out_grads2.T)  # n x n


class SLinearVExt(Operation):
    @staticmethod
    def batch_grads_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        return batch_grads.sum(tuple(range(1, in_data.ndim-1)))

    @staticmethod
    def batch_grads_bias(module, out_grads):
        return out_grads.sum(tuple(range(1, out_grads.ndim-1)))

    @staticmethod
    def batch_grads_kron_weight(
        module: nn.Module, in_data: torch.Tensor, out_grads: torch.Tensor
    ):
        in_data = in_data.sum(tuple(range(1, in_data.ndim-1)))
        out_grads = out_grads.mean(tuple(range(1, out_grads.ndim-1)))
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        return batch_grads

    @staticmethod
    def batch_grads_kron_bias(module, out_grads):
        return out_grads.sum(tuple(range(1, out_grads.ndim-1)))

    @staticmethod
    def cov_diag_weight(module, in_data, out_grads):
        # efficient reduction for augmentation and weight-sharing
        in_data = in_data.mean(tuple(range(1, in_data.ndim-1, 1)))
        out_grads = out_grads.sum(tuple(range(1, out_grads.ndim-1, 1)))
        batch_grads = torch.matmul(
            out_grads.unsqueeze(-1), in_data.unsqueeze(-2)
        )
        return batch_grads.square().sum(dim=0)

    @staticmethod
    def cov_diag_bias(module, out_grads):
        return out_grads.sum(tuple(range(1, out_grads.ndim-1)))

    @staticmethod
    def cov_kron_A(module, in_data):
        dim = np.prod(in_data.shape[1:-1])

        if CASE == 'reduce':
            in_data = in_data.mean(tuple(range(1, in_data.ndim-1, 1))) * sqrt(dim)
        elif CASE == 'expand':
            in_data = in_data.flatten(end_dim=-2) / sqrt(dim)

        m = torch.matmul(in_data.T, in_data)

        return m

    @staticmethod
    def cov_kron_B(module, out_grads):
        deriv = module.regression._deriv

        out_grads = out_grads @ deriv.T

        if CASE == 'reduce':
            out_grads = out_grads.sum(tuple(range(1, out_grads.ndim-1, 1)))
        elif CASE == 'expand':
            out_grads = out_grads.flatten(end_dim=-2)
        m = torch.matmul(out_grads.T, out_grads)

        return m

    @staticmethod
    def gram_A(module, in_data1, in_data2):
        dim = np.prod(in_data1.shape[1:-1])
        assert dim == np.prod(in_data2.shape[1:-1])
        if in_data1.ndim > 2:
            in_data1 = in_data1.mean(tuple(range(1, in_data1.ndim-1))) * sqrt(dim)
            in_data2 = in_data2.mean(tuple(range(1, in_data2.ndim-1))) * sqrt(dim)
        return torch.matmul(in_data1, in_data2.T)  # n x n

    @staticmethod
    def gram_B(module, out_grads1, out_grads2):
        if out_grads1.ndim > 2:
            out_grads1 = out_grads1.sum(tuple(range(1, out_grads1.ndim-1)))
            out_grads2 = out_grads2.sum(tuple(range(1, out_grads2.ndim-1)))
        return torch.matmul(out_grads1, out_grads2.T)  # n x n


